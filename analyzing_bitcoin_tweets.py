# -*- coding: utf-8 -*-
"""Analyzing_bitcoin_tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RRcisCdIUW-YfZO9eCRil8RRu33m4Hu0
"""

# install spacy for natural language processing
!pip install spacy
!pip install -U spacy
!pip install -U spacy-lookups-data
!python -m spacy download en_core_web_sm

# Import the required libraries
import pandas as pd
import numpy as np 
import re
import matplotlib.pyplot as plt

# Import packages of nltk
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from html import unescape   #throw 'trash' words
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

import spacy
import string

# Machine learning libraries
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

# Libraries to evaluate the model
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# English stowords
stop_words = set(stopwords.words('english'))

# Import Wordcloud and textblob for sentiment analysis
from wordcloud import WordCloud, STOPWORDS
plt.style.use('fivethirtyeight')
from textblob import TextBlob

from google.colab import drive
drive.mount('/content/gdrive')

df = pd.read_csv("/content/gdrive/My Drive/bitcoin_ml_algo.csv")

df

# Create a function for a basic text cleaning
def cleanTxt(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text)          # Removing @mentions
    text = re.sub('#[A-Za-z0-9_]+', '', text)          # Removing "#" hash tag
    text = re.sub('RT[\s]+', '', text)     # Removing RT
    text = re.sub(r"\S*https?:\S*", "", text)  # Removing hyperling
    text = re.sub('amp','', text)      # Removing amp symbol
    text = re.sub('&gt;&gt;','', text)
    text = re.sub('gt','', text)    
    text = re.sub('\n', '', text)    # Removing special characters
    text = re.sub(r'[^\w\s]', '', text)   # Removing puncuations  
    text = re.sub(r'\d+', '', text)       # Removing digits
    text = re.sub('_', '', text)
    text = re.sub('\xa0','', text)
    text = text.lower()
  
    return text

df['tweets'] = df['tweets'].apply(cleanTxt)
df

#Create a function to get the polarity
def getPolarity(text):
    return TextBlob(text).sentiment.polarity
def getSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity


#Create a column of sentiment polarity
df['polarity'] = df['tweets'].apply(getPolarity)
df['subjectivity'] = df['tweets'].apply(getSubjectivity)

df

# Define polarity analysis score
def getAnalysis(score):
    if score < 0: 
      return 'negative'
    elif score == 0:
      return 'neutral'
    else:
      return 'positive'

df['analysis'] = df['polarity'].apply(getAnalysis)
df

#pos_tweets = df['tweets'][df['analysis']== 'positive']
#neg_tweets = df['tweets'][df['analysis']== 'negative']
#pos_tweets.str.cat(sep= " ")

# Plot the polarity and subjectivity
plt.figure(figsize=(14,8))
for i in range(0, df.shape[0]):
      plt.scatter(df['polarity'][i], df['subjectivity'][i], color= 'Blue' )
  
plt.title('Sentiment Analysis Scatter Plot')
plt.xlabel('Polarity')
plt.ylabel('Subjectivity')

plt.show()

Positive = df[df['analysis'] =='positive']
print(str(Positive.shape[0] / (df.shape[0]) * 100) + '% of positive tweets')
Pos = Positive.shape[0] / df.shape[0] * 100

Negative = df[df['analysis'] =='negative']
print(str(Negative.shape[0] / (df.shape[0]) * 100) + '% of negative tweets')
Neg = Negative.shape[0] / df.shape[0] * 100

Neutral = df[df['analysis'] =='neutral']
print(str(Neutral.shape[0] / (df.shape[0]) * 100) + '% of neutral tweets')
Neut = Neutral.shape[0] / df.shape[0] * 100

explode = (0,0.1,0)
labels = 'Positive', 'Negative', 'Neutral'
sizes = [Pos, Neg, Neut]
colors = ['lime', 'red', 'cyan']

plt.pie(sizes, explode= explode, colors = colors, autopct= '%1.1f%%',startangle= 120 )
plt.legend(labels , loc= (-0.05,0.05), shadow= True)
plt.axis('equal')
plt.savefig('Sentiment Analysis Pie Chart.png')

polarity = df['polarity']
subjectivity = df['subjectivity']

correlation = polarity .corr(subjectivity, method= 'spearman')
correlation

df.to_csv("/content/gdrive/My Drive/polarity_bitcoin.csv", index= False)

"""CREATE A NAIVE BAYES ALGORITHM

"""

text= df['tweets']
sent= df['analysis']

cleaned_data=[]
for i in range(len(text)):
   tweet=re.sub('[^a-zA-Z]',' ',text.iloc[i])
   tweet=tweet.lower().split()
   tweet=[lemmatizer.lemmatize(word) for word in tweet if (word not in stop_words)]
   tweet=' '.join(tweet)
   cleaned_data.append(tweet)
print(cleaned_data)

# Transform tweets into a vector on the basis of words frequency
cv=CountVectorizer()
X_fin=cv.fit_transform(cleaned_data).toarray()
X_fin

#sentiment_ordering = ['negative', 'neutral', 'positive']
#sent = sent.apply(lambda x: sentiment_ordering.index(x))

model = MultinomialNB()
model

# Train-test split procedure
text_train,text_test,sent_train,sent_test=train_test_split(X_fin,sent,test_size=0.3, random_state=10)
model.fit(text_train,sent_train)

#Test model's accuracy
sent_pred=model.predict(text_test)
accuracy_score(sent_test, sent_pred, normalize= True )

# classification report of the model
cf=classification_report(sent_test,sent_pred)
print(cf)

"""**BOOST THE ACCURACY OF THE MODEL WITH PIPELINE AND TFID VECTORIZER**"""

X = df['tweets']
Y = df['analysis']
X_train,X_test, Y_train,Y_test=train_test_split(X, Y,test_size=0.3, random_state= 3)

cv=CountVectorizer(stop_words= stop_words)
X_train_counts = cv.fit_transform(X_train)

Tfid = TfidfTransformer()
X_train_tfid = Tfid.fit_transform(X_train_counts)

text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words=stop_words, smooth_idf= True, sublinear_tf= True)),
                     ('clf' , MultinomialNB())])

text_clf.fit(X_train , Y_train)

predictions = text_clf.predict(X_test)
predictions

# Confusion matrix
confusion_matrix(Y_test, predictions)

print(metrics.classification_report(Y_test,predictions))

sent_train.unique()

# Extract an ordered array of unique labels.
from sklearn.utils.multiclass import unique_labels

unique_labels(Y_test)

#combine sentiment labels with the confusion matrix

def plot(X_test, X_pred):
    labels = unique_labels(X_test)
    column = [f'Predicted {label}' for label in labels]
    indices = [f'Actual {label}' for label in labels]

    table = pd.DataFrame(confusion_matrix(Y_test, predictions),
                         columns= column, index= indices )
    
    return table

plot(Y_test, predictions)

#library for informative statistical graphs
import seaborn as sns

def plot2(X_test, X_pred):
    labels = unique_labels(sent_test)
    column = [f'{label}' for label in labels]
    indices = [f'{label}' for label in labels]

    table = pd.DataFrame(confusion_matrix(Y_test, predictions),
                         columns= column, index= indices, )
    
    return sns.heatmap(table, annot= True, fmt= 'd',
                       cmap= 'cividis', linewidths= 2,  
                       annot_kws={'fontsize': 20,
                                   'fontweight':'bold',
                                   'fontfamily':'serif'} )

plot2(Y_test, predictions)
plt.xlabel('Predicted labels')
plt.ylabel('Actual labels')